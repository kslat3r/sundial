{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "df955ce39d0f31d56d4bb2fe0a613e5326ba60723fd33d8303a3aede8f65715c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Date      Time Global_active_power Global_reactive_power Voltage  \\\n",
       "0  16/12/2006  17:24:00               4.216                 0.418  234.84   \n",
       "1  16/12/2006  17:25:00               5.360                 0.436  233.63   \n",
       "2  16/12/2006  17:26:00               5.374                 0.498  233.29   \n",
       "3  16/12/2006  17:27:00               5.388                 0.502  233.74   \n",
       "4  16/12/2006  17:28:00               3.666                 0.528  235.68   \n",
       "\n",
       "  Global_intensity Sub_metering_1 Sub_metering_2  Sub_metering_3  \n",
       "0             18.4            0.0            1.0            17.0  \n",
       "1             23.0            0.0            1.0            16.0  \n",
       "2             23.0            0.0            2.0            17.0  \n",
       "3             23.0            0.0            1.0            17.0  \n",
       "4             15.8            0.0            1.0            17.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Time</th>\n      <th>Global_active_power</th>\n      <th>Global_reactive_power</th>\n      <th>Voltage</th>\n      <th>Global_intensity</th>\n      <th>Sub_metering_1</th>\n      <th>Sub_metering_2</th>\n      <th>Sub_metering_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>16/12/2006</td>\n      <td>17:24:00</td>\n      <td>4.216</td>\n      <td>0.418</td>\n      <td>234.84</td>\n      <td>18.4</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>16/12/2006</td>\n      <td>17:25:00</td>\n      <td>5.360</td>\n      <td>0.436</td>\n      <td>233.63</td>\n      <td>23.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>16.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16/12/2006</td>\n      <td>17:26:00</td>\n      <td>5.374</td>\n      <td>0.498</td>\n      <td>233.29</td>\n      <td>23.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16/12/2006</td>\n      <td>17:27:00</td>\n      <td>5.388</td>\n      <td>0.502</td>\n      <td>233.74</td>\n      <td>23.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16/12/2006</td>\n      <td>17:28:00</td>\n      <td>3.666</td>\n      <td>0.528</td>\n      <td>235.68</td>\n      <td>15.8</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>17.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "# import packages\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "# read the dataset into python\n",
    "df = pd.read_csv('data/household_power_consumption.csv', delimiter=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of rows and columns after removing missing values: (2049280, 2)\nThe time series starts from:  2006-12-16 17:24:00\nThe time series ends on:  2010-12-11 23:59:00\n"
     ]
    }
   ],
   "source": [
    "# This code is copied from https://towardsdatascience.com/time-series-analysis-visualization-forecasting-with-lstm-77a905180eba\n",
    "# with a few minor changes.\n",
    "#\n",
    "df['date_time'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "df['Global_active_power'] = pd.to_numeric(df['Global_active_power'], errors='coerce')\n",
    "df = df.dropna(subset=['Global_active_power'])\n",
    "\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "\n",
    "df = df.loc[:, ['date_time', 'Global_active_power']]\n",
    "df.sort_values('date_time', inplace=True, ascending=True)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print('Number of rows and columns after removing missing values:', df.shape)\n",
    "print('The time series starts from: ', df['date_time'].min())\n",
    "print('The time series ends on: ', df['date_time'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2049280 entries, 0 to 2049279\nData columns (total 2 columns):\n #   Column               Dtype         \n---  ------               -----         \n 0   date_time            datetime64[ns]\n 1   Global_active_power  float64       \ndtypes: datetime64[ns](1), float64(1)\nmemory usage: 31.3 MB\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            date_time  Global_active_power\n",
       "0 2006-12-16 17:24:00                4.216\n",
       "1 2006-12-16 17:25:00                5.360\n",
       "2 2006-12-16 17:26:00                5.374\n",
       "3 2006-12-16 17:27:00                5.388\n",
       "4 2006-12-16 17:28:00                3.666"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date_time</th>\n      <th>Global_active_power</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2006-12-16 17:24:00</td>\n      <td>4.216</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2006-12-16 17:25:00</td>\n      <td>5.360</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2006-12-16 17:26:00</td>\n      <td>5.374</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2006-12-16 17:27:00</td>\n      <td>5.388</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2006-12-16 17:28:00</td>\n      <td>3.666</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test dates: 2010-12-05 00:00:00 to 2010-12-11 23:59:00\nValidation dates: 2010-11-21 00:00:00 to 2010-12-04 23:59:00\nTrain dates: 2006-12-16 17:24:00 to 2010-11-20 23:59:00\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation and test datasets.\n",
    "# Since it's timeseries we should do it by date.\n",
    "test_cutoff_date = df['date_time'].max() - timedelta(days=7)\n",
    "val_cutoff_date = test_cutoff_date - timedelta(days=14)\n",
    "\n",
    "df_test = df[df['date_time'] > test_cutoff_date]\n",
    "df_val = df[(df['date_time'] > val_cutoff_date) & (df['date_time'] <= test_cutoff_date)]\n",
    "df_train = df[df['date_time'] <= val_cutoff_date]\n",
    "\n",
    "#check out the datasets\n",
    "print('Test dates: {} to {}'.format(df_test['date_time'].min(), df_test['date_time'].max()))\n",
    "print('Validation dates: {} to {}'.format(df_val['date_time'].min(), df_val['date_time'].max()))\n",
    "print('Train dates: {} to {}'.format(df_train['date_time'].min(), df_train['date_time'].max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ts_files(dataset, \n",
    "                    start_index, \n",
    "                    end_index, \n",
    "                    history_length, \n",
    "                    step_size, \n",
    "                    target_step, \n",
    "                    num_rows_per_file, \n",
    "                    data_folder):\n",
    "    assert step_size > 0\n",
    "    assert start_index >= 0\n",
    "    \n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "    \n",
    "    time_lags = sorted(range(target_step+1, target_step+history_length+1, step_size), reverse=True)\n",
    "    col_names = [f'x_lag{i}' for i in time_lags] + ['y']\n",
    "    start_index = start_index + history_length\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_step\n",
    "    \n",
    "    rng = range(start_index, end_index)\n",
    "    num_rows = len(rng)\n",
    "    num_files = math.ceil(num_rows/num_rows_per_file)\n",
    "    \n",
    "    # for each file.\n",
    "    print(f'Creating {num_files} files.')\n",
    "    for i in range(num_files):\n",
    "        filename = f'{data_folder}/ts_file{i}.pkl'\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'{filename}')\n",
    "            \n",
    "        # get the start and end indices.\n",
    "        ind0 = i*num_rows_per_file\n",
    "        ind1 = min(ind0 + num_rows_per_file, end_index)\n",
    "        data_list = []\n",
    "        \n",
    "        # j in the current timestep. Will need j-n to j-1 for the history. And j + target_step for the target.\n",
    "        for j in range(ind0, ind1):\n",
    "            indices = range(j-1, j-history_length-1, -step_size)\n",
    "            data = dataset[sorted(indices) + [j+target_step]]\n",
    "            \n",
    "            # append data to the list.\n",
    "            data_list.append(data)\n",
    "\n",
    "        df_ts = pd.DataFrame(data=data_list, columns=col_names)\n",
    "        df_ts.to_pickle(filename)\n",
    "            \n",
    "    return len(col_names)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating 158 files.\n",
      "data/ts_data/ts_file0.pkl\n",
      "data/ts_data/ts_file10.pkl\n",
      "data/ts_data/ts_file20.pkl\n",
      "data/ts_data/ts_file30.pkl\n",
      "data/ts_data/ts_file40.pkl\n",
      "data/ts_data/ts_file50.pkl\n",
      "data/ts_data/ts_file60.pkl\n",
      "data/ts_data/ts_file70.pkl\n",
      "data/ts_data/ts_file80.pkl\n",
      "data/ts_data/ts_file90.pkl\n",
      "data/ts_data/ts_file100.pkl\n",
      "data/ts_data/ts_file110.pkl\n",
      "data/ts_data/ts_file120.pkl\n",
      "data/ts_data/ts_file130.pkl\n",
      "data/ts_data/ts_file140.pkl\n",
      "data/ts_data/ts_file150.pkl\n",
      "CPU times: user 18min 31s, sys: 1min 4s, total: 19min 35s\n",
      "Wall time: 20min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "global_active_power = df_train['Global_active_power'].values\n",
    "\n",
    "# Scaled to work with Neural networks.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "global_active_power_scaled = scaler.fit_transform(global_active_power.reshape(-1, 1)).reshape(-1, )\n",
    "\n",
    "history_length = 7*24*60  # The history length in minutes.\n",
    "step_size = 10  # The sampling rate of the history. Eg. If step_size = 1, then values from every minute will be in the history.\n",
    "                #                                       If step size = 10 then values every 10 minutes will be in the history.\n",
    "target_step = 10  # The time step in the future to predict. Eg. If target_step = 0, then predict the next timestep after the end of the history period.\n",
    "                  #                                             If target_step = 10 then predict 10 timesteps the next timestep (11 minutes after the end of history).\n",
    "\n",
    "# The csv creation returns the number of rows and number of features. We need these values below.\n",
    "num_timesteps = create_ts_files(global_active_power_scaled,\n",
    "                                start_index=0,\n",
    "                                end_index=None,\n",
    "                                history_length=history_length,\n",
    "                                step_size=step_size,\n",
    "                                target_step=target_step,\n",
    "                                num_rows_per_file=128*100,\n",
    "                                data_folder='data/ts_data')\n",
    "\n",
    "# I found that the easiest way to do time series with tensorflow is by creating pandas files with the lagged time steps (eg. x{t-1}, x{t-2}...) and \n",
    "# the value to predict y = x{t+n}. We tried doing it using TFRecords, but that API is not very intuitive and lacks working examples for time series.\n",
    "# The resulting file using these parameters is over 17GB. If history_length is increased, or  step_size is decreased, it could get much bigger.\n",
    "# Hard to fit into laptop memory, so need to use other means to load the data from the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# So we can handle loading the data in chunks from the hard drive instead of having to load everything into memory.\n",
    "# \n",
    "# The reason we want to do this is so we can do custom processing on the data that we are feeding into the LSTM.\n",
    "# LSTM requires a certain shape and it is tricky to get it right.\n",
    "#\n",
    "class TimeSeriesLoader:\n",
    "    def __init__(self, ts_folder, filename_format):\n",
    "        self.ts_folder = ts_folder\n",
    "        \n",
    "        # find the number of files.\n",
    "        i = 0\n",
    "        file_found = True\n",
    "        while file_found:\n",
    "            filename = self.ts_folder + '/' + filename_format.format(i)\n",
    "            file_found = os.path.exists(filename)\n",
    "            if file_found:\n",
    "                i += 1\n",
    "                \n",
    "        self.num_files = i\n",
    "        self.files_indices = np.arange(self.num_files)\n",
    "        self.shuffle_chunks()\n",
    "        \n",
    "    def num_chunks(self):\n",
    "        return self.num_files\n",
    "    \n",
    "    def get_chunk(self, idx):\n",
    "        assert (idx >= 0) and (idx < self.num_files)\n",
    "        \n",
    "        ind = self.files_indices[idx]\n",
    "        filename = self.ts_folder + '/' + filename_format.format(ind)\n",
    "        df_ts = pd.read_pickle(filename)\n",
    "        num_records = len(df_ts.index)\n",
    "        \n",
    "        features = df_ts.drop('y', axis=1).values\n",
    "        target = df_ts['y'].values\n",
    "        \n",
    "        # reshape for input into LSTM. Batch major format.\n",
    "        features_batchmajor = np.array(features).reshape(num_records, -1, 1)\n",
    "        return features_batchmajor, target\n",
    "    \n",
    "    # this shuffles the order the chunks will be outputted from get_chunk.\n",
    "    def shuffle_chunks(self):\n",
    "        np.random.shuffle(self.files_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_folder = 'data/ts_data'\n",
    "filename_format = 'ts_file{}.pkl'\n",
    "tss = TimeSeriesLoader(ts_folder, filename_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Keras model.\n",
    "# Use hyperparameter optimization if you have the time.\n",
    "\n",
    "ts_inputs = tf.keras.Input(shape=(num_timesteps, 1))\n",
    "\n",
    "# units=10 -> The cell and hidden states will be of dimension 10.\n",
    "#             The number of parameters that need to be trained = 4*units*(units+2)\n",
    "x = layers.LSTM(units=10)(ts_inputs)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(1, activation='linear')(x)\n",
    "model = tf.keras.Model(inputs=ts_inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the training configuration.\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 1008, 1)]         0         \n_________________________________________________________________\nlstm (LSTM)                  (None, 10)                480       \n_________________________________________________________________\ndropout (Dropout)            (None, 10)                0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 11        \n=================================================================\nTotal params: 491\nTrainable params: 491\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch #0\n",
      "100/100 [==============================] - 26s 248ms/step - loss: 0.0129 - mse: 0.0129\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 0.0174 - mse: 0.0174\n",
      "100/100 [==============================] - 23s 231ms/step - loss: 0.0106 - mse: 0.0106\n",
      "100/100 [==============================] - 23s 231ms/step - loss: 0.0173 - mse: 0.0173\n",
      "100/100 [==============================] - 24s 245ms/step - loss: 0.0210 - mse: 0.0210\n",
      "100/100 [==============================] - 24s 241ms/step - loss: 0.0146 - mse: 0.0146\n",
      "100/100 [==============================] - 23s 227ms/step - loss: 0.0096 - mse: 0.0096\n",
      "100/100 [==============================] - 23s 229ms/step - loss: 0.0103 - mse: 0.0103\n",
      "100/100 [==============================] - 23s 228ms/step - loss: 0.0089 - mse: 0.0089\n",
      "100/100 [==============================] - 22s 224ms/step - loss: 0.0105 - mse: 0.0105\n",
      "100/100 [==============================] - 22s 225ms/step - loss: 0.0111 - mse: 0.0111\n",
      "100/100 [==============================] - 23s 227ms/step - loss: 0.0144 - mse: 0.0144\n",
      "100/100 [==============================] - 24s 241ms/step - loss: 0.0133 - mse: 0.0133\n",
      "100/100 [==============================] - 23s 225ms/step - loss: 0.0129 - mse: 0.0129\n",
      "100/100 [==============================] - 23s 232ms/step - loss: 0.0066 - mse: 0.0066\n",
      "100/100 [==============================] - 23s 232ms/step - loss: 0.0029 - mse: 0.0029\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.0090 - mse: 0.0090\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.0090 - mse: 0.0090\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0087 - mse: 0.0087\n",
      "100/100 [==============================] - 23s 226ms/step - loss: 0.0071 - mse: 0.0071\n",
      "100/100 [==============================] - 22s 225ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 23s 229ms/step - loss: 0.0078 - mse: 0.0078\n",
      "100/100 [==============================] - 27s 272ms/step - loss: 0.0108 - mse: 0.0108\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.0075 - mse: 0.0075\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 0.0066 - mse: 0.0066\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.0058 - mse: 0.0058\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 0.0062 - mse: 0.0062\n",
      "100/100 [==============================] - 24s 243ms/step - loss: 0.0060 - mse: 0.0060\n",
      "100/100 [==============================] - 24s 239ms/step - loss: 0.0069 - mse: 0.0069\n",
      "100/100 [==============================] - 23s 231ms/step - loss: 0.0064 - mse: 0.0064\n",
      "100/100 [==============================] - 23s 228ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.0069 - mse: 0.0069\n",
      "100/100 [==============================] - 31s 311ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.0097 - mse: 0.0097\n",
      "100/100 [==============================] - 24s 241ms/step - loss: 0.0068 - mse: 0.0068\n",
      "100/100 [==============================] - 23s 233ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.0051 - mse: 0.0051\n",
      "100/100 [==============================] - 28s 280ms/step - loss: 0.0070 - mse: 0.0070\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 29s 295ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 0.0051 - mse: 0.0051\n",
      "100/100 [==============================] - 25s 246ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 27s 274ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.0062 - mse: 0.0062\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 0.0076 - mse: 0.0076\n",
      "100/100 [==============================] - 28s 278ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 0.0092 - mse: 0.0092\n",
      "100/100 [==============================] - 23s 232ms/step - loss: 0.0059 - mse: 0.0059\n",
      "100/100 [==============================] - 23s 233ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 23s 233ms/step - loss: 0.0071 - mse: 0.0071\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0069 - mse: 0.0069\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 24s 245ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 24s 239ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 24s 241ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 24s 241ms/step - loss: 0.0054 - mse: 0.0054\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 0.0069 - mse: 0.0069\n",
      "100/100 [==============================] - 25s 246ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0075 - mse: 0.0075\n",
      "100/100 [==============================] - 24s 235ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 23s 234ms/step - loss: 0.0055 - mse: 0.0055\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0057 - mse: 0.0057\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 24s 239ms/step - loss: 0.0057 - mse: 0.0057\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0019 - mse: 0.0019\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 24s 239ms/step - loss: 0.0053 - mse: 0.0053\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0022 - mse: 0.0022\n",
      "100/100 [==============================] - 24s 240ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0062 - mse: 0.0062\n",
      "100/100 [==============================] - 24s 240ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 24s 243ms/step - loss: 0.0058 - mse: 0.0058\n",
      "100/100 [==============================] - 24s 241ms/step - loss: 0.0062 - mse: 0.0062\n",
      "100/100 [==============================] - 24s 242ms/step - loss: 0.0033 - mse: 0.0033\n",
      "100/100 [==============================] - 24s 241ms/step - loss: 0.0065 - mse: 0.0065\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.0072 - mse: 0.0072\n",
      "100/100 [==============================] - 24s 239ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 24s 239ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0080 - mse: 0.0080\n",
      "100/100 [==============================] - 24s 241ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 24s 242ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 23s 234ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 23s 235ms/step - loss: 0.0055 - mse: 0.0055\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.0068 - mse: 0.0068\n",
      "100/100 [==============================] - 23s 233ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0021 - mse: 0.0021\n",
      "100/100 [==============================] - 23s 234ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 24s 242ms/step - loss: 0.0033 - mse: 0.0033\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0021 - mse: 0.0021\n",
      "100/100 [==============================] - 27s 273ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.0032 - mse: 0.0032\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.0061 - mse: 0.0061\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 0.0063 - mse: 0.0063\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 4.5639e-04 - mse: 4.5639e-04\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 25s 255ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 0.0055 - mse: 0.0055\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.0061 - mse: 0.0061\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 0.0059 - mse: 0.0059\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 0.0058 - mse: 0.0058\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.0021 - mse: 0.0021\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 0.0023 - mse: 0.0023\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 0.0016 - mse: 0.0016\n",
      "100/100 [==============================] - 26s 255ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 26s 255ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 0.0054 - mse: 0.0054\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 0.0059 - mse: 0.0059\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 0.0028 - mse: 0.0028\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 0.0061 - mse: 0.0061\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 0.0056 - mse: 0.0056\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.0034 - mse: 0.0034\n",
      "CPU times: user 2h 19min 10s, sys: 27min 52s, total: 2h 47min 3s\n",
      "Wall time: 1h 6min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train in batch sizes of 128.\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 1\n",
    "NUM_CHUNKS = tss.num_chunks()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('epoch #{}'.format(epoch))\n",
    "    for i in range(NUM_CHUNKS):\n",
    "        X, y = tss.get_chunk(i)\n",
    "        \n",
    "        # model.fit does train the model incrementally. ie. Can call multiple times in batches.\n",
    "        # https://github.com/keras-team/keras/issues/4446\n",
    "        model.fit(x=X, y=y, batch_size=BATCH_SIZE)\n",
    "        \n",
    "    # shuffle the chunks so they're not in the same order next time around.\n",
    "    tss.shuffle_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating 1 files.\ndata/ts_val_data/ts_file0.pkl\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the validation set.\n",
    "#\n",
    "# Create the validation CSV like we did before with the training.\n",
    "global_active_power_val = df_val['Global_active_power'].values\n",
    "global_active_power_val_scaled = scaler.transform(global_active_power_val.reshape(-1, 1)).reshape(-1, )\n",
    "\n",
    "history_length = 7*24*60  # The history length in minutes.\n",
    "step_size = 10  # The sampling rate of the history. Eg. If step_size = 1, then values from every minute will be in the history.\n",
    "                #                                       If step size = 10 then values every 10 minutes will be in the history.\n",
    "target_step = 10  # The time step in the future to predict. Eg. If target_step = 0, then predict the next timestep after the end of the history period.\n",
    "                  #                                             If target_step = 10 then predict 10 timesteps the next timestep (11 minutes after the end of history).\n",
    "\n",
    "# The csv creation returns the number of rows and number of features. We need these values below.\n",
    "num_timesteps = create_ts_files(global_active_power_val_scaled,\n",
    "                                start_index=0,\n",
    "                                end_index=None,\n",
    "                                history_length=history_length,\n",
    "                                step_size=step_size,\n",
    "                                target_step=target_step,\n",
    "                                num_rows_per_file=128*100,\n",
    "                                data_folder='data/ts_val_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "validation mean squared error: 0.44101810579895095\nvalidation baseline mean squared error: 0.428345914375\n"
     ]
    }
   ],
   "source": [
    "# If we assume that the validation dataset can fit into memory we can do this.\n",
    "df_val_ts = pd.read_pickle('data/ts_val_data/ts_file0.pkl')\n",
    "\n",
    "\n",
    "features = df_val_ts.drop('y', axis=1).values\n",
    "features_arr = np.array(features)\n",
    "\n",
    "# reshape for input into LSTM. Batch major format.\n",
    "num_records = len(df_val_ts.index)\n",
    "features_batchmajor = features_arr.reshape(num_records, -1, 1)\n",
    "\n",
    "\n",
    "y_pred = model.predict(features_batchmajor).reshape(-1, )\n",
    "y_pred = scaler.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1 ,)\n",
    "\n",
    "y_act = df_val_ts['y'].values\n",
    "y_act = scaler.inverse_transform(y_act.reshape(-1, 1)).reshape(-1 ,)\n",
    "\n",
    "print('validation mean squared error: {}'.format(mean_squared_error(y_act, y_pred)))\n",
    "\n",
    "#baseline\n",
    "y_pred_baseline = df_val_ts['x_lag11'].values\n",
    "y_pred_baseline = scaler.inverse_transform(y_pred_baseline.reshape(-1, 1)).reshape(-1 ,)\n",
    "print('validation baseline mean squared error: {}'.format(mean_squared_error(y_act, y_pred_baseline)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}